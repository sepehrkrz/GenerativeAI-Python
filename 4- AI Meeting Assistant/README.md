
Business AI Meeting Companion STT
=================================

![cognitiveclass.ai logo](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-WD0231EN-SkillsNetwork/IDSN-logo.png)

Introduction
------------

Consider you're attending a business meeting where all conversations are being captured by an advanced AI application. This application not only transcribes the discussions with high accuracy but also provides a concise summary of the meeting, emphasizing the key points and decisions made.

In our project, we'll use OpenAI's Whisper to transform speech into text. Next, we'll use IBM Watson's AI to summarize and find key points. We'll make an app with Hugging Face Gradio as the user interface.

### Learning Objectives

After finishing this lab, you will able to:

-   Create a Python script to generate text using a model from the Hugging Face Hub, identify some key parameters that influence the model's output, and have a basic understanding of how to switch between different LLM models.
-   Use OpenAI's Whisper technology to convert lecture recordings into text, accurately.
-   Implement IBM Watson's AI to effectively summarize the transcribed lectures and extract key points.
-   Create an intuitive and user-friendly interface using Hugging Face Gradio, ensuring ease of use for students and educators.

![langchain](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-GPXX0V2VEN/images/DALL%C2%B7E%202024-02-29%2014.12.41%20-%20In%20a%20minimalist%20meeting%20room%20with%20a%20large%2C%20plain%20round%20table%2C%20a%20small%20and%20simple%20digital%20display%20is%20mounted%20on%20a%20white%20wall.%20The%20display%20shows%20%27Key%20Po.webp)

Generated by DALLE-3

Preparing the environment
-------------------------

Let's start with setting up the environment by creating a Python virtual environment and installing the required libraries, using the following commands in the terminal:

```bash
pip3 install virtualenv 
virtualenv my_env # create a virtual environment my_env
source my_env/bin/activate # activate my_env
```
Then, install the required libraries in the environment (this will take time ☕️☕️):

``` bash
# installing required libraries in my_env
pip install transformers==4.35.2 torch==2.1.1 gradio==4.17.0 langchain==0.0.343 ibm_watson_machine_learning==1.0.335 huggingface-hub==0.19.4
```
We need to install  `ffmpeg`  to be able to work with audio files in python.

```bash
sudo apt update
```

Then run:

```bash
sudo apt install ffmpeg -y
```

# Step 1: Speech-to-Text

Initially, we want to create a simple speech-to-text Python file using OpenAI Whisper.

You can test the sample audio file  **Sample voice  [link to download.](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-GPXX04C6EN/Testing%20speech%20to%20text.mp3)**

Create and open a Python file and call it  `simple_speech2text.py`  by clicking the link below:

Open  **simple_speech2text.py**  in IDE

Let's download the file first (you can do it manually, then drag and drop it into the file environment).

``` python
import requests

# URL of the audio file to be downloaded
url = "https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-GPXX04C6EN/Testing%20speech%20to%20text.mp3"

# Send a GET request to the URL to download the file
response = requests.get(url)

# Define the local file path where the audio file will be saved
audio_file_path = "downloaded_audio.mp3"

# Check if the request was successful (status code 200)
if response.status_code == 200:
    # If successful, write the content to the specified local file path
    with open(audio_file_path, "wb") as file:
        file.write(response.content)
    print("File downloaded successfully")
else:
    # If the request failed, print an error message
    print("Failed to download the file")
```
Run the Python file to test it.

``` bash
python3 simple_speech2text.py
```

You should see the downloaded audio file in the file explorer.

![langchain](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-GPXX04C6EN/images/whisper_download.jpg.jpg)

Next, implement OpenAI Whisper for transcribing voice to speech.

You can override the previous code in the Python file.

``` python
import torch
from transformers import pipeline

# Initialize the speech-to-text pipeline from Hugging Face Transformers
# This uses the "openai/whisper-tiny.en" model for automatic speech recognition (ASR)
# The `chunk_length_s` parameter specifies the chunk length in seconds for processing
pipe = pipeline(
  "automatic-speech-recognition",
  model="openai/whisper-tiny.en",
  chunk_length_s=30,
)

# Define the path to the audio file that needs to be transcribed
sample = 'downloaded_audio.mp3'

# Perform speech recognition on the audio file
# The `batch_size=8` parameter indicates how many chunks are processed at a time
# The result is stored in `prediction` with the key "text" containing the transcribed text
prediction = pipe(sample, batch_size=8)["text"]

# Print the transcribed text to the console
print(prediction)
```
Run the Python file and you will get the output.

``` bash
python3 simple_speech2text.py
```

![langchain](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-GPXX04C6EN/images/whisper%20simple%20result.jpg)

In the next step, we will utilize  `Gradio`  for creating interface for our app.

# Gradio interface

## Creating a simple demo

Through this project, we will create different LLM applications with Gradio interface. Let's get familiar with Gradio by creating a simple app:

Still in the  `project`  directory, create a Python file and name it  `hello.py`.

Open  `hello.py`, paste the following Python code and save the file.

``` python
import gradio as gr

def greet(name):
    return "Hello " + name + "!"

demo = gr.Interface(fn=greet, inputs="text", outputs="text")

demo.launch(server_name="0.0.0.0", server_port= 7860)
```

The above code creates a  **gradio.Interface**  called  `demo`. It wraps the  `greet`  function with a simple text-to-text user interface that you could interact with.

The  **gradio.Interface**  class is initialized with 3 required parameters:

-   fn: the function to wrap a UI around
-   inputs: which component(s) to use for the input (e.g. “text”, “image” or “audio”)
-   outputs: which component(s) to use for the output (e.g. “text”, “image” or “label”)

The last line  `demo.launch()`  launches a server to serve our  `demo`.

## Launching the demo app

Now go back to the terminal and make sure that the  `my_env`  virtual environment name is displayed at the beginning of the line. Run the following command to execute the Python script.

``` bash
python3 hello.py
```

As the Python code is served by a local host, click on the button below and you will be able to see the simple application we just created. Feel free to play around with the input and output of the web app!

Click here to see the application:

Web application

You should see the following, here we entered the name Bob:  
![](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-GPXX0ZLFEN/images/bob.png)

If you finish playing with the app and want to exit,  **press  `Ctrl+c`  in the terminal and close the application tab**.

If you wish to learn a little bit more about customization in Gradio, you are invited to take the guided project called  **Bring your Machine Learning model to life with Gradio**. You can find it under  **Courses & Projects**  on  [cognitiveclass.ai](https://cognitiveclass.ai/)!

For the rest of this project, we will use Gradio as an interface for LLM apps.

# Step 2: Creating audio transcription app

Create a new python file  `speech2text_app.py`

Open  **speech2text_app.py**  in IDE

#### Exercise: Complete the  `transcript_audio`  function.

From the step1: fill the missing parts in  `transcript_audio`  function.

``` python
import torch
from transformers import pipeline
import gradio as gr

# Function to transcribe audio using the OpenAI Whisper model
def transcript_audio(audio_file):
    # Initialize the speech recognition pipeline
    pipe = pipeline(
        "automatic-speech-recognition",
        model="openai/whisper-tiny.en",
        chunk_length_s=30,
    )
    # Transcribe the audio file and return the result
    result = pipe(audio_file, batch_size=8)["text"]
    return result

# Set up Gradio interface
audio_input = gr.Audio(sources="upload", type="filepath")  # Audio input
output_text = gr.Textbox()  # Text output

# Create the Gradio interface with the function, inputs, and outputs
iface = gr.Interface(fn=transcript_audio, 
                     inputs=audio_input, outputs=output_text, 
                     title="Audio Transcription App",
                     description="Upload the audio file")

# Launch the Gradio app
iface.launch(server_name="0.0.0.0", server_port=7860)
```
Then, run your app:

``` bash
python3 speech2text_app.py
```

And start the app:

Web application

You can download the sample audio file we've provided by right-clicking on it in the file explorer and selecting "Download." Once downloaded, you can upload this file to the app. Alternatively, feel free to choose and upload any MP3 audio file from your local computer.

The result will be:

![langchain](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-GPXX04C6EN/images/speech2text_app.jpg)

**Press Ctrl + C to stop the application.**

Step 3: Integrating LLM: Using Llama 2 in WatsonX as LLM
========================================================

Running simple LLM
------------------

Let's start by generating text with LLMs. Create a Python file and name it `simple_llm.py`. You can proceed by clicking the link below or by referencing the accompanying image.

 Open **simple_llm.py** in IDE

In case, you want to use Llama 2 as an LLM instance, you can follow the instructions below:

> IBM WatsonX utilizes various language models, including Llama 2 by Meta, which is currently the strongest open-source language model.

Here's how the code works:

1.  **Setting up credentials**: The credentials needed to access IBM's services are pre-arranged by the Skills Network team, so you don't have to worry about setting them up yourself.

2.  **Specifying parameters**: The code then defines specific parameters for the language model. 'MAX_NEW_TOKENS' sets the limit on the number of words the model can generate in one go. 'TEMPERATURE' adjusts how creative or predictable the generated text is.

3.  **Setting up Llama 2 model**: Next, the LLAMA2 model is set up using a model ID, the provided credentials, chosen parameters, and a project ID.

4.  **Creating an object for Llama 2**: The code creates an object named 'llm', which is used to interact with the LLAMA2 model. A model object, `LLAMA2_model`, is created using the Model class, which is initialized with a specific model ID, credentials, parameters, and project ID. Then, an instance of WatsonxLLM is created with `LLAMA2_model` as an argument, initializing the language model hub 'llm' object.

5.  **Generating and printing response**: Finally, 'llm' is used to generate a response to the question, "How to read a book effectively?" The response is then printed out.

``` python
from ibm_watson_machine_learning.foundation_models import Model
from ibm_watson_machine_learning.foundation_models.extensions.langchain import WatsonxLLM
from ibm_watson_machine_learning.metanames import GenTextParamsMetaNames as GenParams

my_credentials = {
    "url"    : "https://us-south.ml.cloud.ibm.com"
}

params = {
        GenParams.MAX_NEW_TOKENS: 800, # The maximum number of tokens that the model can generate in a single run.
        GenParams.TEMPERATURE: 0.1,   # A parameter that controls the randomness of the token generation. A lower value makes the generation more deterministic, while a higher value introduces more randomness.
    }

LLAMA2_model = Model(
        model_id= 'meta-llama/llama-2-70b-chat', 
        credentials=my_credentials,
        params=params,
        project_id="skills-network",  
        )

llm = WatsonxLLM(LLAMA2_model)  

print(llm("How to read a book effectively?"))
```


You can then run this script in the terminal using the following command:
```bash
python3 simple_llm.py
```

Upon running the script, you should see the generated text in your terminal, as shown below:

![](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-GPXX04C6EN/images/llama2_read_book.jpg)

You can see how watsonx Llama 2 provides a good answer.